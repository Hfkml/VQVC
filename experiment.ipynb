{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from creapy import creapy\n",
    "import plotly\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "import librosa\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy.io.wavfile import write\n",
    "import utils\n",
    "from models import SynthesizerTrn\n",
    "from speaker_encoder.voice_encoder import SpeakerEncoder\n",
    "from wavlm import WavLM, WavLMConfig\n",
    "from datetime import datetime\n",
    "import IPython.display as ipd \n",
    "import json\n",
    "import soundfile as sf\n",
    "from praatio import textgrid as tg\n",
    "from praatio.utilities.constants import Interval\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the models and helper functions\n",
    "This includes setting location of the downloaded checkpoint off WavLM and the FreeVC model to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wavlm_large_path = 'wavlm/WavLM-Large.pt'\n",
    "wavlm_large_path = '/home/lameris/CreakVC/wavlm/WavLM-Large.pt'\n",
    "freevc_chpt_path = 'logs/libri_train_only/VQVC.pth'\n",
    "\n",
    "# --- Argument creator ---\n",
    "def arg_creator(source, target, outpath, creak, cpps, h1h2, pitch, h1a3, pitch_var):\n",
    "    return [source, target, outpath, creak, cpps, h1h2, pitch, h1a3, pitch_var]\n",
    "\n",
    "# --- Load WavLM model ---\n",
    "def get_cmodel():\n",
    "    checkpoint = torch.load(wavlm_large_path)\n",
    "    cfg = WavLMConfig(checkpoint['cfg'])\n",
    "    cmodel = WavLM(cfg)\n",
    "    cmodel.load_state_dict(checkpoint['model'])\n",
    "    cmodel.eval()\n",
    "    return cmodel.to(device)\n",
    "\n",
    "# --- Convert to tensor ---\n",
    "def to_tensor(x, audio_len, device='cpu'):\n",
    "    if isinstance(x, (int, float)):\n",
    "        return torch.full((audio_len,), x, dtype=torch.float32, device=device)\n",
    "    return x.to(device)\n",
    "\n",
    "# --- Generic voice quality adjustment ---\n",
    "def apply_vq(args, audio_len, scaling_factor, time_range, target_vals, sr=16000, device=device):\n",
    "    if scaling_factor == 0:\n",
    "        return args\n",
    "\n",
    "    start_frame, end_frame = (0, audio_len) if time_range == (0, -1) else (\n",
    "        int(time_range[0] * sr // 320),\n",
    "        min(int(time_range[1] * sr // 320), audio_len)\n",
    "    )\n",
    "\n",
    "    creak, cpps, h1h2, h1a3 = map(lambda x: to_tensor(x, audio_len, device), [args[3], args[4], args[5], args[7]])\n",
    "    for param, target in zip([creak, cpps, h1h2, h1a3], target_vals):\n",
    "        param[start_frame:end_frame] += (target[start_frame:end_frame] - param[start_frame:end_frame]) * scaling_factor\n",
    "\n",
    "    return [args[0], args[1], args[2], creak, cpps, h1h2, args[6], h1a3, args[8]]\n",
    "\n",
    "# --- Combined VQ workflow ---\n",
    "def combined_vq(current_args, pitch, pitch_var, audio, sr=16000, device=device):\n",
    "    tensor = utils.get_content(cmodel, torch.tensor(audio, dtype=torch.float32).unsqueeze(0).to(device))\n",
    "    tensor_size = tensor.shape[-1] + 1\n",
    "\n",
    "    def get_segments(vq_name, default_intensity=1):\n",
    "        ranges_input = input(f\"Enter ranges for {vq_name} (e.g., (0,0.4),(0.5,1.9)) or press Enter for full clip: \").strip()\n",
    "\n",
    "        # If user presses Enter, assume full clip\n",
    "        if not ranges_input:\n",
    "            parsed_ranges = [(0, -1)]\n",
    "        else:\n",
    "            time_ranges = re.findall(r'\\(\\s*([\\d.-]+)\\s*,\\s*([\\d.-]+)\\s*\\)', ranges_input)\n",
    "            parsed_ranges = [(float(start), float(end)) for start, end in time_ranges]\n",
    "\n",
    "        segments = []\n",
    "        for time_range in parsed_ranges:\n",
    "            while True:\n",
    "                intensity_input = input(f\"Select intensity for {vq_name} {time_range} (0-5, default {default_intensity}): \").strip()\n",
    "                if not intensity_input:\n",
    "                    intensity = default_intensity\n",
    "                    break\n",
    "                try:\n",
    "                    intensity = float(intensity_input)\n",
    "                    if 0 <= intensity <= 5:\n",
    "                        break\n",
    "                    else:\n",
    "                        print(\"Intensity must be between 0 and 5.\")\n",
    "                except ValueError:\n",
    "                    print(\"Please enter a number between 0 and 5.\")\n",
    "            segments.append((time_range, intensity))\n",
    "\n",
    "        return segments\n",
    "\n",
    "    breathy_segments = get_segments(\"breathiness\")\n",
    "    creaky_segments = get_segments(\"creakiness\")\n",
    "    nasal_segments = get_segments(\"nasality\")\n",
    "\n",
    "    # Define target values\n",
    "    targets = {\n",
    "        \"breathy\": [torch.full((tensor_size,), -2., device=device),\n",
    "                    torch.full((tensor_size,), -1., device=device),\n",
    "                    torch.full((tensor_size,), 3., device=device),\n",
    "                    torch.full((tensor_size,), 3., device=device)],\n",
    "        \"creaky\": [torch.full((tensor_size,), 3., device=device),\n",
    "                   torch.full((tensor_size,), -1., device=device),\n",
    "                   torch.full((tensor_size,), -2., device=device),\n",
    "                   torch.full((tensor_size,), -2., device=device)],\n",
    "        \"nasal\": [torch.full((tensor_size,), 0., device=device),\n",
    "                  torch.full((tensor_size,), 1., device=device),\n",
    "                  torch.full((tensor_size,), -3., device=device),\n",
    "                  torch.full((tensor_size,), 3., device=device)]\n",
    "    }\n",
    "\n",
    "    for segments, vq_name in [(nasal_segments, \"nasal\"), (breathy_segments, \"breathy\"), (creaky_segments, \"creaky\")]:\n",
    "        for time_range, intensity in segments:\n",
    "            if intensity > 0:\n",
    "                current_args = apply_vq(current_args, tensor_size, intensity, time_range, targets[vq_name], sr, device)\n",
    "\n",
    "    # Update pitch & variance\n",
    "    current_args[6] = pitch\n",
    "    current_args[8] = pitch_var\n",
    "\n",
    "    # Print averages\n",
    "    stats = [\"creak\", \"cpps\", \"h1h2\", \"pitch\", \"h1a3\", \"pitch_var\"]\n",
    "    for i, stat in enumerate(stats, start=3):\n",
    "        val = current_args[i]\n",
    "        print(f\"Current average {stat}: {torch.mean(val).item() if isinstance(val, torch.Tensor) else val}\")\n",
    "\n",
    "    convert(current_args)\n",
    "    return current_args\n",
    "\n",
    "# --- Reset args ---\n",
    "def reset_args(args):\n",
    "    return [args[0], args[1], args[2]] + [0]*6\n",
    "\n",
    "# --- Timestamp ---\n",
    "def generate_timestamp():\n",
    "    now = datetime.now()\n",
    "    return now.strftime(\"%Y%m%d_%H%M%S_\") + str(int(now.microsecond / 1000)).zfill(2)\n",
    "\n",
    "# --- Convert ---\n",
    "def convert(args):\n",
    "    print(\"Converting...\")\n",
    "    wav_tgt, _ = librosa.load(args[1], sr=hps.data.sampling_rate)\n",
    "    wav_tgt, _ = librosa.effects.trim(wav_tgt, top_db=20)\n",
    "    g_tgt = torch.from_numpy(smodel.embed_utterance(wav_tgt)).unsqueeze(0).to(device)\n",
    "\n",
    "    wav_src, _ = librosa.load(args[0], sr=hps.data.sampling_rate, mono=True)\n",
    "    wav_src = torch.from_numpy(wav_src).unsqueeze(0).to(device)\n",
    "    c = utils.get_content(cmodel, wav_src)\n",
    "\n",
    "    for dim in [c.shape[-1], c.shape[-1]-1]:\n",
    "        basic_tensor = torch.zeros((1,1,dim), device=device)\n",
    "        try:\n",
    "            tgt_audio = net_g.infer(c, g=g_tgt,\n",
    "                                    creaks=basic_tensor+args[3],\n",
    "                                    cpps=basic_tensor+args[4],\n",
    "                                    h1h2s=basic_tensor+args[5],\n",
    "                                    pitches=basic_tensor+args[6],\n",
    "                                    h1a3s=basic_tensor+args[7],\n",
    "                                    pitch_vars=basic_tensor+args[8])\n",
    "            break\n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "    tgt_audio = tgt_audio[0][0].cpu().detach().numpy()\n",
    "    timestamp = generate_timestamp()\n",
    "    print(timestamp)\n",
    "    ipd.display(ipd.Audio(tgt_audio, rate=hps.data.sampling_rate))\n",
    "    write(args[2], hps.data.sampling_rate, tgt_audio)\n",
    "    return tgt_audio\n",
    "\n",
    "# --- Create VQ tensor from segments ---\n",
    "def create_vq_tensor(wav_len, vq_segments, sr):\n",
    "    end_frame = wav_len // 320\n",
    "    vqs = torch.zeros((1,1,end_frame), dtype=torch.float32)\n",
    "    for start, end, val in [(int(s*sr//320), int(e*sr//320), v) for s,e,v in vq_segments]:\n",
    "        vqs[0,0,start:end] = val\n",
    "    return vqs\n",
    "\n",
    "# --- JSON to TextGrid ---\n",
    "def json_to_textgrid(json_path, textgrid_path):\n",
    "    with open(json_path) as f:\n",
    "        data = json.load(f)\n",
    "    start_speech, end_speech = data['segments'][0]['start'], data['segments'][-1]['end']\n",
    "    timestamps = [word for seg in data['segments'] for word in seg['words']]\n",
    "\n",
    "    tg_obj = tg.Textgrid()\n",
    "    tier = tg.IntervalTier('word', [], start_speech, end_speech)\n",
    "    for i, t in enumerate(timestamps):\n",
    "        start = timestamps[i-1]['end']+0.01 if i>0 else t['start']\n",
    "        end = t['end']\n",
    "        tier.insertEntry(Interval(start, end, t['word']))\n",
    "    tg_obj.addTier(tier)\n",
    "    tg_obj.save(textgrid_path, format='long_textgrid', includeBlankSpaces=False)\n",
    "    return timestamps\n",
    "\n",
    "# --- Initialize ---\n",
    "hps = utils.get_hparams_from_file('configs/freevc.json')\n",
    "net_g = SynthesizerTrn(hps.data.filter_length//2+1, hps.train.segment_size//hps.data.hop_length, **hps.model)\n",
    "utils.load_checkpoint(freevc_chpt_path, net_g, optimizer=None, strict=True)\n",
    "cmodel = get_cmodel()\n",
    "smodel = SpeakerEncoder('speaker_encoder/ckpt/pretrained_bak_5805000.pt', device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the arguments\n",
    "Here we set the following arguments for the creaky voice conversion:\n",
    "\n",
    "1. **source_path** indicates the audio file of which we want the linguistic content.\n",
    "2. **target_path** indicates the audio file containing speech of the target speaker.\n",
    "3. **outpath** is the location where the converted audio will be saved.\n",
    "4. **average_feature** is the initial feature value over the complete utterance that will be supplied to the model \n",
    "\n",
    "We also create the output folder specified in the arguments\n",
    "\n",
    "\n",
    "## Instructions\n",
    "1. Select the source path that you want to convert by uncommenting it. \n",
    "2. Select your desired feature values. I recommend starting with all zeros, except for pitch where I recommend starting at -1.\n",
    "3. Listen to the original audio in the cell below the manipulation cell.\n",
    "4. Perform the conversion.\n",
    "5. Decide if you like the prosody.\n",
    "6. If not, change the feature values in the coarse-grained editing cell.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulation cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_path = \"/shared/lameris/spoken_stereoset/speech/gender/02273_en-US-ChristopherNeural.wav\"\n",
    "\n",
    "\n",
    "target_path = \"/home/lameris/CreakVC/speech_continuation/concatenated_christopher_44s.wav\"\n",
    "outpath = f\"./data/experiment/{source_path.split('/')[-1]}\"\n",
    "\n",
    "average_creak = 0\n",
    "average_cpps = 0\n",
    "average_h1h2 = 0\n",
    "average_pitch = -1.5\n",
    "average_h1a3 = 0\n",
    "average_pitch_var = 0\n",
    "\n",
    "sr=16000\n",
    "args = arg_creator(source_path, target_path, outpath, average_creak, average_cpps, average_h1h2, average_pitch, average_h1a3, average_pitch_var)\n",
    "os.makedirs('/'.join(args[2].split('/')[:-1]), exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "src_audio = sf.read(source_path)[0]\n",
    "ipd.display(ipd.Audio(src_audio, rate=16000))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform the conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_audio = convert(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coarse-grained editing for finding pitch and pitch variation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_creak = 0\n",
    "average_cpps = 0\n",
    "average_h1h2 = 0\n",
    "average_h1a3 = 0\n",
    "average_pitch = -1.5\n",
    "average_pitch_var = 3\n",
    "\n",
    "orig_args = arg_creator(source_path, target_path, outpath, average_creak, average_cpps, average_h1h2, average_pitch, average_h1a3, average_pitch_var)\n",
    "converted_audio = convert(orig_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run WhisperX\n",
    "* We run speech recognition to transcribe the generated utterance\n",
    "* We convert the timestamps from json to TextGrid in order to use prepare them for CreaPy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run whisper\n",
    "import json\n",
    "import os\n",
    "from praatio import textgrid as tg\n",
    "from praatio.utilities.constants import Interval\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "!whisperx \"/shared/lameris/spoken_stereoset/speech/gender/02273_en-US-ChristopherNeural.wav\" --model distil-medium.en --output_dir data/experiment --language en \n",
    "\n",
    "#read json file with the start and end times of the words\n",
    "json_path = \"data/experiment/\" + args[0].split('/')[-1].replace(\".wav\", \".json\")\n",
    "tg_path = \"data/experiment/\" + source_path.split('/')[-1][:-4] + '.TextGrid'\n",
    "timestamps = json_to_textgrid(json_path, tg_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run CreapPy\n",
    "This enables us to quantify and visualize the creak probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_pred, sr = creapy.process_file(textgrid_path=tg_path, audio_path=f\"./data/experiment/{source_path.split('/')[-1]}\")\n",
    "X_test['h1h2'] = np.nan_to_num(X_test['h1h2'])\n",
    "X_test['h1h2'] = np.convolve(X_test['h1h2'], np.ones(20)/20, mode='same')\n",
    "y_pred_smoothed = np.convolve(y_pred, np.ones(20)/20, mode='same')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the audio\n",
    "Get the durations from here for the fine-grained editing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = creapy.plot(X_test, y_pred_smoothed, sr, words=timestamps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-grained editing\n",
    "\n",
    "Edit the features according to the following syntax:\n",
    "\n",
    "\n",
    "You only need to change the times and feature values!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combined_vq needs: args, pitch, pitch_var, audio\n",
    "\n",
    "combined = combined_vq(args, -1.5, 2, converted_audio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = combined_vq(args, -1.5, 0, converted_audio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_creak = 0\n",
    "average_cpps = 0\n",
    "average_h1h2 = 0\n",
    "average_h1a3 = 0\n",
    "average_pitch = -1.5\n",
    "average_pitch_var = 3\n",
    "\n",
    "orig_args = arg_creator(source_path, target_path, outpath, average_creak, average_cpps, average_h1h2, average_pitch, average_h1a3, average_pitch_var)\n",
    "converted_audio = convert(orig_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breathy = [source_path, target_path, outpath, -1, -1, 2, -1.5, 2, 1]\n",
    "creaky = [source_path, target_path, outpath, 2, -.5, -1, -2, -1, -2]\n",
    "high_pitch_var = [source_path, target_path, outpath, -3, 0, -.5, -1.5, -1, 4]\n",
    "tense = [source_path, target_path, outpath, -1, 1, 2, -1.5, -2, 0]\n",
    "nasal = [source_path, target_path, outpath, -1, 1, -3, -1.5, 3, 0]\n",
    "\n",
    "print('--------------Breathy----------------')\n",
    "converted_breathy_audio = convert(breathy)\n",
    "print('--------------Creaky----------------')\n",
    "converted_creaky_audio = convert(creaky)\n",
    "print('--------- Pitch-variation----------------')\n",
    "converted_high_pitch_var_audio = convert(high_pitch_var)\n",
    "print('--------------Tense----------------')\n",
    "converted_tense_audio = convert(tense)\n",
    "print('--------------Nasal----------------')\n",
    "converted_nasal_audio = convert(nasal)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "updated_CreakVC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
